# -*- coding: utf-8 -*-
"""Ejercicio_Titanic.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FAMrnqJgpIFbBTOQ2GUrT3f2BNu8Mbgf

# Práctica de ML

Este es mi notebook relacionado con el ejercicio desarrollado en el curso de Udemy "Python Total".
"""

# Importar librerias
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
from sklearn import tree

# Cargar archivo - Metodo google drive
from google.colab import drive
drive.mount('/content/drive')
datos = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/Data_Set_Titanic.csv")

# Cargar archivo - Metodo pc
# from google.colab import files
# uploaded = files.upload()
# datos = pd.read_csv("Data_Set_Titanic.csv")

# Verificar los datos de los archivos
datos.head()

# Verificar las estadisticas de los datos
datos.describe()

# Verificar el impacto de algun dato
sns.countplot(x="Survived", data=datos, hue="Sex")

# Verificar datos vacios
datos.isna().sum()

# Rellenar datos vacios con el promedio
datos["Age"].mean()
datos["Age"] = datos["Age"].fillna(datos["Age"].mean())
datos["Age"]

# Quitar datos
datos = datos.drop(["Name","PassengerId", "Ticket", "Cabin", "Embarked"], axis = 1)

# Verificar nuevamente que no haya vacios, en caso de que sean muy pocos se pueden elimina
datos.isna().sum()
datos = datos.dropna()
datos.isna().sum()

# Convertir male y female a datos numericos
dummies_sex = pd.get_dummies(datos["Sex"], drop_first=True)
# drop_first evita multicollinearity

# Añadir los datos de dummies_sex a datos
datos = datos.join(dummies_sex)
datos = datos.drop(["Sex"], axis = 1)

# Ver la correlacion de los datos
sns.heatmap(datos.corr(), annot=True, cmap="YlGnBu")

# Ver la correlacion de los datos
sns.countplot(x="Survived", data = datos, hue="Pclass")

# Comenzara entrenar el modelo
# Separar los datos del resultado
X = datos.drop(["Survived"], axis=1)
y = datos["Survived"]

from sklearn.model_selection import train_test_split

X_ent, X_pru, y_ent, y_pru = train_test_split(X,y, test_size=.2)

from sklearn.linear_model import LogisticRegression
modelo = LogisticRegression(max_iter=1000)
modelo.fit(X_ent, y_ent)

predicciones = modelo.predict(X_pru)

from sklearn.metrics import accuracy_score
accuracy_score(y_pru, predicciones)

from sklearn.metrics import classification_report
print(classification_report(y_pru, predicciones))

pd.DataFrame(confusion_matrix(y_pru, predicciones), columns=["Pred: No", "Pred: Si"], index=["Real: No", "Real: Si"])

# Ver datos de X para inventar persona
X.head()

# Inventar persona
Nueva_persona = [2,25,0,0,30.0000, False]
prediccion = modelo.predict([Nueva_persona])
if prediccion[0] == 1:
  print("Hubieras sobrevivido")
else:
  print("No hubieras sobrevivido")